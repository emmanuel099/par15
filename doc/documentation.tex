\documentclass[11pt,a4paper]{article}
\usepackage[top=2cm, left=2cm, right=2cm, bottom=3cm]{geometry}
\usepackage[utf8]{inputenc}

\title{2D Stencil - Parallel Approaches}
\author{
  Emmanuel Pescosta\\
  1326934 
  \and
  Philipp Paris\\
  1325664
}
\date{\today}

\begin{document}

\maketitle

\section{Problem Definition}

% stencil definition

\section{Sequential}
\subsection{Memory-infficient Implementation}
% with temporary matrix
\subsection{Memory-efficient Implementation}
% with single vector buffer

\section{Cilk}

% task-parallel formulation of the stencil update, compare with openmp solution

\section{OpenMP}
\subsection{Classical Implementation}
% temporary matrix, not thread local
\subsection{Row-wise Implementation}
% row-wise update: thread local vs non thread local 
\subsection{Column-wise Implementation}
% column-wise update: thread local vs non thread local 

\section{MPI}
% matrix is distributed as n/r x m/c
% give a theoretical speedup estimate: How should r and c be chosen for best performance
\subsection{Blocking Boundary Exchange}
% sendrecv
\subsection{Point-To-Point Boundary Exchange}
%non-blocking point-to-point
\subsection{One-Sided Boundary Exchange}
%one sided

\section{Testing}
% ctest

\section{Results}

\subsection{Saturn}
Saturn is a shared memory parallel computer with 48 AMD CPU cores.

\begin{table}[!ht]
  \label{tab:saturn} 
  \caption{Hard- and software configuration of Saturn}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      CPUs & 4 AMD Opteron 6168 (12 cores, 1.9 GHz, 12 MB cache)\\\hline
      Main Memory & 128 GB DDR3-1333\\\hline
      Operating system & Linux 64 bit (Debian Testing)\\\hline
      Compiler & gcc (Debian 5.2.1-23) 5.2.1 20151028\\\hline
      Cilk & 5.4.6\\\hline
      MPI & Open MPI 1.8.4\\\hline
    \end{tabular}
  \end{center}
\end{table}

The hard- and software configuration of the test machine can be seen in table \ref{tab:saturn}. As compiler options we used \verb|-Ofast|, \verb|-std=gnu99| and \verb|-msse2| in addition to the default compiler flags provided by the individual frameworks. We enabled \verb|-DNDEBUG| and thread pinning for OpenMP, by exporting \verb|OMP_PROC_BIND=true|, while we did the benchmarks.\\
\\
On Saturn we compared the following stencil implementations:
\subsubsection{Sequential}
\subsubsection{OpenMP}
\subsubsection{Cilk}
\subsubsection{MPI}

\subsection{Jupiter}
Jupiter is a distributed memory parallel computer cluster with 36 computing nodes interconnected by a InfiniBand network. Each computing node has 16 AMD CPU cores.

\begin{table}[!ht]
  \label{tab:jupiter} 
  \caption{Hard- and software configuration of Jupiter}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      Computing nodes & 36\\\hline
      CPUs per node & 2 AMD Opteron 6134 (8 cores, 2.3 GHz, 12 MB cache)\\\hline
      Main Memory per node & 32 GB DDR3-1333\\\hline
      Interconnection network & QDR InfiniBand and Gigabit Ethernet\\\hline
      Operating system & Linux 64 bit (CentOS 6)\\\hline
      Compiler & gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-16)\\\hline
      MPI & MPICH 3.0.4\\\hline
    \end{tabular}    
  \end{center}
\end{table}

The hard- and software configuration of the test machine can be seen in table \ref{tab:jupiter}. As compiler options we used \verb|-O3|, \verb|-std=gnu99| and \verb|-msse2| in addition to the default compiler flags provided by the individual frameworks. We enabled \verb|-DNDEBUG| while we did the benchmarks. The host file was generated by \verb|echo -e jupiter{0..35}\\n > hosts| and passed on to \verb|mpiexec| via the \verb|hostfile| flag.\\
\\
On Jupiter we compared the following stencil implementations:
\subsubsection{Sequential}
\subsubsection{MPI}

\section{Conclusion}
% comparison between openmp, cilk and mpi

\end{document}
