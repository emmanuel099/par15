\documentclass[11pt,a4paper]{article}
\usepackage[top=2cm, left=2cm, right=2cm, bottom=3cm]{geometry}
\usepackage[utf8]{inputenc}

\title{2D Stencil - Parallel Approaches}
\author{
  Emmanuel Pescosta\\
  1326934 
  \and
  Philipp Paris\\
  1325664
}
\date{\today}

\begin{document}

\maketitle

\section{Preliminaries}

\subsection{Stencil Computation}
Stencil computations update each value of an n-dimensional array as a function of the corresponding neighbourhood values. Equation \ref{eq:stencil} shows an example neighbourhood function for a 2-dimensional array.
\begin{equation}\label{eq:stencil}
 A[i,j] = \frac{A[i-1,j] + A[i+1,j] + A[i,j-1] + A[i,j+1]}{4}
\end{equation}
In a single stencil computation this process of updating all values of the array is usually repeated a certain number of times, called iterations. \\
Special measures have to be taken to update the boundary values of the array, which lack one or more neighbour values: there exist many different ways and in this project we will simply skip the updating of the boundary values.

\subsection{Aim of the project}
In this project we compare different implementations of a Five-Point-2D-Stencil (equation \ref{eq:stencil}) in their memory consumption and runtime. We will look at sequential and parallel implementations using Cilk, OpenMP and MPI.\\
The aim is to achieve the best possible speedup of the calculation using parallel algorithms in comparison to the sequential implementation while keeping the memory consumption as low as possible. Furthermore we want to to analyze the impact of the memory usage on the runtime of the calculation.\\
Another intention is to analyze cache-aware and cache-unaware implementations of the stencil-computation in their runtime. 

\section{Sequential}
\subsection{Memory-infficient Implementation}
% with temporary matrix
\subsection{Memory-efficient Implementation}
% with single vector buffer

\section{Cilk}

% task-parallel formulation of the stencil update, compare with openmp solution

\section{OpenMP}
\subsection{Classical Implementation}
% temporary matrix, not thread local
\subsection{Row-wise Implementation}
% row-wise update: thread local vs non thread local 
\subsection{Column-wise Implementation}
% column-wise update: thread local vs non thread local 

\section{MPI}
% matrix is distributed as n/r x m/c
% give a theoretical speedup estimate: How should r and c be chosen for best performance
\subsection{Blocking Boundary Exchange}
% sendrecv
\subsection{Point-To-Point Boundary Exchange}
%non-blocking point-to-point
\subsection{One-Sided Boundary Exchange}
%one sided

\section{Testing}
% ctest

\section{Results}

\subsection{Saturn}
Saturn is a shared memory parallel computer with 48 AMD CPU cores.

\begin{table}[!ht]
  \label{tab:saturn} 
  \caption{Hard- and software configuration of Saturn}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      CPUs & 4 AMD Opteron 6168 (12 cores, 1.9 GHz, 12 MB cache)\\\hline
      Main Memory & 128 GB DDR3-1333\\\hline
      Operating system & Linux 64 bit (Debian Testing)\\\hline
      Compiler & gcc (Debian 5.2.1-23) 5.2.1 20151028\\\hline
      Cilk & 5.4.6\\\hline
      MPI & Open MPI 1.8.4\\\hline
    \end{tabular}
  \end{center}
\end{table}

The hard- and software configuration of the test machine can be seen in table \ref{tab:saturn}. As compiler options we used \verb|-Ofast|, \verb|-std=gnu99| and \verb|-msse2| in addition to the default compiler flags provided by the individual frameworks. We enabled \verb|-DNDEBUG| and thread pinning for OpenMP, by exporting \verb|OMP_PROC_BIND=true|, while we did the benchmarks.\\
\\
On Saturn we compared the following stencil implementations:
\subsubsection{Sequential}
\subsubsection{OpenMP}
\subsubsection{Cilk}
\subsubsection{MPI}

\subsection{Jupiter}
Jupiter is a distributed memory parallel computer cluster with 36 computing nodes interconnected by a InfiniBand network. Each computing node has 16 AMD CPU cores.

\begin{table}[!ht]
  \label{tab:jupiter} 
  \caption{Hard- and software configuration of Jupiter}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      Computing nodes & 36\\\hline
      CPUs per node & 2 AMD Opteron 6134 (8 cores, 2.3 GHz, 12 MB cache)\\\hline
      Main Memory per node & 32 GB DDR3-1333\\\hline
      Interconnection network & QDR InfiniBand and Gigabit Ethernet\\\hline
      Operating system & Linux 64 bit (CentOS 6)\\\hline
      Compiler & gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-16)\\\hline
      MPI & MPICH 3.0.4\\\hline
    \end{tabular}    
  \end{center}
\end{table}

The hard- and software configuration of the test machine can be seen in table \ref{tab:jupiter}. As compiler options we used \verb|-O3|, \verb|-std=gnu99| and \verb|-msse2| in addition to the default compiler flags provided by the individual frameworks. We enabled \verb|-DNDEBUG| while we did the benchmarks. The host file was generated by \verb|echo -e jupiter{0..35}\\n > hosts| and passed on to \verb|mpiexec| via the \verb|hostfile| flag.\\
\\
On Jupiter we compared the following stencil implementations:
\subsubsection{Sequential}
\subsubsection{MPI}

\section{Conclusion}
% comparison between openmp, cilk and mpi

\end{document}
