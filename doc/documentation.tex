\documentclass[11pt,a4paper]{article}
\usepackage[top=2cm, left=2cm, right=2cm, bottom=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{url}
\usepackage{algorithm2e}
\usepackage{algpseudocode}

\title{2D Stencil - Parallel Approaches}
\author{
  Emmanuel Pescosta\\
  1326934 
  \and
  Philipp Paris\\
  1325664
}
\date{\today}

\SetKwRepeat{Parallel}{\#pragma omp parallel} {end}

\begin{document}

\maketitle

\section{Preliminaries}

\subsection{Stencil Computation}
Stencil computations update each value of an n-dimensional array as a function of the corresponding neighbourhood values. Equation \ref{eq:stencil} shows an example neighbourhood function for a 2-dimensional array.
\begin{equation}\label{eq:stencil}
 A[i,j] = \frac{A[i-1,j] + A[i+1,j] + A[i,j-1] + A[i,j+1]}{4}
\end{equation}
In a single stencil computation this process of updating all values of the array is usually repeated a certain number of times, called iterations. \\
Special measures have to be taken to update the boundary values of the array, which lack one or more neighbour values: there exist many different ways and in this project we will simply skip the updating of the boundary values.

\subsection{Aim of the project}
In this project we compare different implementations of a Five-Point-2D-Stencil (equation \ref{eq:stencil}) in their memory consumption and runtime. We will look at sequential and parallel implementations using Cilk, OpenMP and MPI.\\
The aim is to achieve the best possible speedup of the calculation using parallel algorithms in comparison to the sequential implementation while keeping the memory consumption as low as possible. Furthermore we want to to analyze the impact of the memory usage on the runtime of the calculation.\\
Another intention is to analyze cache-aware and cache-unaware implementations of the stencil-computation in their runtime.

\section{Sequential}
\subsection{Memory-inefficient Implementation}
This implementation uses a temporary 2-dimensional array to store the result of the stencil calculation. After the execution of one iteration, the temporary and original matrix are switched, the temporary matrix storing the latest values and the original matrix the future result of the iteration.\\
The matrix is iterated in a row-wise manner to keep the amount of cache-misses low.\\
In this implementation we can see, that the memory consumption is in order of $O(2 \cdot m \cdot n)$, where $m$ denotes the number of rows and $n$ the number of columns of the input matrix.\\
Algorithm \ref{algo:seq_tmp_matrix} shows the workings of this implementation in Pseudo-Code.

\begin{algorithm}[H] \label{algo:seq_tmp_matrix}
 \KwData{A: input 2D array, B: temporary 2D array, i: iterations}
 \ForEach{iteration i}{
  \ForEach{row r}{
    \ForEach{column c}{
      B[r,c] = stencil\_five\_point\_kernel(A, r, c)
    }
  }
  exchange A and B
 }
 
 \If{$i \bmod 2 \neq 0$} {
  exchange A and B
 }
 \caption{Sequential 2D stencil with temporary matrix}
\end{algorithm}

\subsection{Memory-efficient Implementation} \label{subsec:seq2}
In contrast to the implementation above we want to keep the memory consumption in this implementation as low as possible. Therefore we buffer only the last row of the matrix instead of the whole array. This gives us a memory consumption of $O(m \cdot n + m)$, where again $m$ denotes the number of rows and $n$ the number of columns.\\

\begin{algorithm}[H] \label{algo:seq_one_vec}
 \KwData{A: input 2D array, B: temporary row array, i: iterations}
 \ForEach{iteration i}{
 
    \ForEach{column c}{
      B[c] = stencil\_five\_point\_kernel(A, first row, c)
    }
 
    \ForEach{row r = first row + 1}{
      \ForEach{column c}{
	value = stencil\_five\_point\_kernel(A, r, c)\\
	A[r - 1, c] = B[c]\\
	B[c] = value
      }
    }
    
    \ForEach{column c}{
      A[last row, c] = B[c]
    }
 }
 
 \caption{Sequential 2D stencil}
\end{algorithm}


\section{Cilk}
In this implementation the $n\times m$ matrix is distributed among $p$ workers so that the calculation one iteration of the stencil update can be done in parallel. This is achieved by splitting the matrix horizontally into $p$ parts, so that each worker is responsible for $\frac{n}{p}$ number of rows (one worker takes more rows if $n \bmod p \neq 0$).\\
To ensure that each worker has the correct non-updated boundary values, each worker calculates at first the first row of the assigned part of the matrix and buffers the calculated values in $B_{worker}$. If all workers have completed this step, the caclulation of the following rows can be started. At the end of the calculation the buffer values $B$ are copied back to the original stencil array.\\
This process is repeated for every iteration.\\
If we ignore the overhead of the cilk instructions the runtime of this algorithm should be in the order of $O(\frac{n \cdot m}{p})$ per iteration, where $m$ denotes the number of columns, $n$ the number of rows and $p$ the number of wokers.

\begin{algorithm}[H] \label{algo:cilk}
 \KwData{A: input 2D array, it: iterations, w: number of workers}
 \ForEach{iteration}{
    \For{i = 0 to w}{
      row = i $\cdot$ rows\_per\_worker\\
      B[i] = cilk\_spawn five\_point\_stencil\_for\_row(A, row)
    }
    cilk\_sync
 
    \For{i = 0 to w}{
      start\_row = i $\cdot$ rows\_per\_worker + 1\\
      cilk\_spawn stencil\_sequential(A, start\_row, rows\_per\_worker)
    }
    cilk\_sync
    
    \For{i = 0 to w}{
      row = i $\cdot$ rows\_per\_worker\\
      stencil\_matrix\_set\_row(A, row, B[i])
    }
 }
 \caption{Cilk 2D stencil}
\end{algorithm}

% task-parallel formulation of the stencil update, compare with openmp solution

\section{OpenMP}
We implemented a memory-efficient and memory-inefficent variation of the stencil algorithm using OpenMP.
Furthermore, to analyze the impact of the cache on the performance, we implemented two versions of these two algorithms: one with a row-wise and the other with a column-wise approach in updating the cells. The performance of the column-wise approach should be worse due to the higher number of cache misses when accessing the array cells (the array is organized in a row-wise manner).\\
The following sections describe the row-wise implementation.

\subsection{Memory-efficient Implementation}
The $n\times m$ matrix is distributed among the available threads p, by splitting the matrix horizontally, assigning each worker $\frac{n}{p}$ rows. Each worker buffers the result of the last row (to ensure that the other threads use the correct boundary) and calculates the stencil update by using the algorithm \ref{algo:seq_one_vec}).\\
At the end of each iteration the last row buffer is copied back to the matrix.\\
The memory consumption of this algorithm is $O(n \cdot m + 2 \cdot p \cdot m)$ and the runtime for one iteration is $O(\frac{m \cdot n}{p})$.\\

\begin{algorithm}[H] \label{algo:cilk}
 \KwData{A: input 2D array, iterations, start\_row, end\_row}
 \Parallel{}{
    \ForEach{iteration}{
	last\_row\_buffer = five\_point\_stencil\_for\_row(A, end\_row)\\
	\#pragma omp barrier

	stencil\_sequential(A, start\_row, end\_row - 1)\\
	
	stencil\_matrix\_set\_row(A, end\_row, last\_row\_buffer)\\
	\#pragma omp barrier
    }
 }
 \caption{OpenMP row-wise memory-efficient stencil}
\end{algorithm}

\subsection{Memory-inefficient Implementation}
Again we distribute the $n\times m$ matrix to each thread by splitting the matrix horizontally and assigning each worker $\frac{n}{p}$ rows. In contrast to the efficient variation, each thread creates in this algorithm a copy of the assigned submatrix, which it then works on. This ensures that the submatrix is stored in the local memory of the corresponding thread, reducing communication overhead.\\
The calculation of the stencil update operation is again done by using algorithm \ref{algo:seq_one_vec}.\\
After the execution of each iteration the boundaries of the submatrix are distributed to the other threads.\\
This gives us a memory consumption of $O(2 \cdot m \cdot n + m \cdot p)$ and a runtime per iteration of $O(\frac{m \cdot n}{p} + 2 \cdot (p-1) \cdot m)$, where $2 \cdot (p-1) \cdot m$ denotes the overhead for the boundary exchange.\\

\begin{algorithm}[H] \label{algo:cilk}
 \KwData{A: input 2D array, iterations, start\_row, end\_row}
 \Parallel{}{
    B = submatrix(A, start\_row - 1, end\_row + 1)\\
    \#pragma omp barrier
    
    \ForEach{iteration}{
	\If{iteration $>$ 1}{
	  exchange boundary rows with upper and lower neighbours\\
	  \#pragma omp barrier
	}
	stencil\_sequential(B, start\_row, end\_row)\\
    } 
 }
 \caption{OpenMP row-wise memory-inefficient stencil}
\end{algorithm}

\section{MPI}
A matrix of size $m * n$, where $m$ denotes the number of rows and $n$ the number of columns, is evenly distributed to all nodes taking part in the MPI communication. All nodes are aligned in a cartesian-grid-like communication topology with the size $c * r$, where $c$ denotes the number of nodes on the x-axis and $r$ the number of nodes on the y-axis. Every node gets a submatrix of size $\tfrac{m}{c} * \tfrac{n}{r}$. The pre-conditions $m \equiv_c 0$ and $n \equiv_r 0$ must hold, otherwise the stencil calculation will be aborted.\\
\\
The stencil computation complexity of each node is $O(\tfrac{m}{c} * \tfrac{n}{r} + 2 * \tfrac{m}{c} + 2 * \tfrac{n}{r})$ for one iteration, where $\tfrac{m}{c} * \tfrac{n}{r}$ comes from the complexity of the sequential stencil operator and $2 * \tfrac{m}{c} + 2 * \tfrac{n}{r}$ is the complexity of the boundary exchange. To achieve the optimal performance, we have to make sure that the boundary between neighbouring nodes is minimized. This can be done by optimizing the dimensions of the cartesian grid in a way so that $\tfrac{m}{c} + \tfrac{n}{r}$ is minimized, or in other words, the width-height-ratio of the given matrix and the cartesian grid should be more or less the same.

\subsection{Blocking Boundary Exchange}

\begin{algorithm}[H] \label{algo:mpi:blocking}
 \KwData{M: matrix, N: neighbours}
 \ForEach{neighbour n in N}{
    send and receive boundary data of neighbour n synchronously\;
 }
 \caption{Synchronous Point-to-Point Communication}
\end{algorithm}

\subsection{Non-Blocking Boundary Exchange}

\begin{algorithm}[H] \label{algo:mpi:nonblocking}
 \KwData{M: matrix, N: neighbours}
 \ForEach{neighbour n in N}{
    send boundary data of M to neighbour n asynchronously\;
    receive boundary data from neighbour n asynchronously\;
 }
 wait until all asynchronous calls are finished\;
 \caption{Asynchronous Point-to-Point Communication}
\end{algorithm}

\subsection{One-Sided Boundary Exchange}
We implemented and benchmarked two different synchronization methods for one-sided communication. Algorithm \ref{algo:mpi:onesided:fence} uses fence synchronization, which uses so-called remote memory access epochs between two \verb|fence| calls, to handle the communication synchronization. Algorithm \ref{algo:mpi:onesided:pscw} uses the post-start-complete-wait synchronization method, which has the advantage over fence synchronization, that the synchronization is done using pre-defined communication groups instead of doing the synchronization over the entire communicator (see \cite{SteveLantz2013} for detailed explanation).\\

\begin{algorithm}[H] \label{algo:mpi:onesided:fence}
 \KwData{W: memory window, M: matrix, N: neighbours}
 fence(W)\;
 \ForEach{neighbour n in N}{
    put boundary data of M to neighbour n\;
 }
 fence(W)\;
 \caption{One-Sided Fence Synchronization}
\end{algorithm}

\begin{algorithm}[H] \label{algo:mpi:onesided:pscw}
 \KwData{W: memory window, G: communication group, M: matrix, N: neighbours}
 post(G, W)\;
 start(G, W)\;
 \ForEach{neighbour n in N}{
    put boundary data of M to neighbour n\;
 }
 complete(W)\;
 wait(W)\;
 \caption{One-Sided Post-Start-Complete-Wait Synchronization}
\end{algorithm}

\section{Testing}
We use data-driven testing to check the correctness of all different stencil implementations. The tests are executed via CTest and some custom CMake commands. Each stencil operator is applied to multiple different matrices, which vary in number of rows and columns, and after 5 iterations the outcome of the algorithm is compared to the expected result. The MPI implementation makes use of the collective operations \verb|MPI_Scatterv| and \verb|MPI_Gatherv|, for distributing and collecting the matrix values, to test the correctness of the distributed stencil implementation.

\section{Results}

\subsection{Saturn}
Saturn is a shared memory parallel computer with 48 AMD CPU cores.

\begin{table}[H]
  \label{tab:saturn} 
  \caption{Hard- and software configuration of Saturn}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      CPUs & 4 AMD Opteron 6168 (12 cores, 1.9 GHz, 12 MB cache)\\\hline
      Main Memory & 128 GB DDR3-1333\\\hline
      Operating system & Linux 64 bit (Debian Testing)\\\hline
      Compiler & gcc (Debian 5.2.1-23) 5.2.1 20151028\\\hline
      Cilk & 5.4.6\\\hline
      MPI & Open MPI 1.8.4\\\hline
    \end{tabular}
  \end{center}
\end{table}

The hard- and software configuration of the test machine can be seen in table \ref{tab:saturn}. As compiler options we used \verb|-Ofast|, \verb|-std=gnu99| and \verb|-msse2| in addition to the default compiler flags provided by the individual frameworks. We enabled \verb|-DNDEBUG| and thread pinning for OpenMP, by exporting \verb|OMP_PROC_BIND=true|, while we did the benchmarks.\\
\\
On Saturn we compared the sequential, OpenMP, Cilk and MPI stencil implementations. We measured the average execution time of each stencil implementation by executing each of them 30 times in a row. The problem sizes are a variation between different matrix sizes and different number of iterations. We took 1000x1000, 2000x2000 and 6000x6000 as matrix sizes for our benchmark, and we applied the stencil operator on each matrix size with 10 and 100 iterations. The benchmark of 6000x6000 with 100 iterations was aborted, because it took far too long to finish.

\begin{figure}[H] 
\caption{Sequential benchmark on Saturn}
\begin{tabular}{cc}
\subcaptionbox{10 Iterations\label{saturn:seq:10}}{\includegraphics[width=0.5\textwidth]{saturn_seq_10.pdf}} &
\subcaptionbox{100 Iterations\label{saturn:seq:100}}{\includegraphics[width=0.5\textwidth]{saturn_seq_100.pdf}}
\end{tabular}
\end{figure}

\begin{figure}[H] 
\caption{OpenMP benchmark on Saturn}
\begin{tabular}{cc}
\subcaptionbox{1000x1000 Matrix with 10 Iterations\label{saturn:openmp:1000:10}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_1000x1000_10.pdf}} &
\subcaptionbox{1000x1000 Matrix with 100 Iterations\label{saturn:openmp:1000:100}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_1000x1000_100.pdf}}\\
\subcaptionbox{2000x2000 Matrix with 10 Iterations\label{saturn:openmp:2000:10}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_2000x2000_10.pdf}} &
\subcaptionbox{2000x2000 Matrix with 100 Iterations\label{saturn:openmp:2000:100}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_2000x2000_100.pdf}}\\
\subcaptionbox{6000x6000 Matrix with 10 Iterations\label{saturn:openmp:6000:10}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_6000x6000_10.pdf}}
\end{tabular}
\end{figure}

\begin{figure}[H] 
\caption{Cilk benchmark on Saturn}
\begin{tabular}{cc}
\subcaptionbox{1000x1000 Matrix with 10 Iterations\label{saturn:cilk:1000:10}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_1000x1000_10.pdf}} &
\subcaptionbox{1000x1000 Matrix with 100 Iterations\label{saturn:cilk:1000:100}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_1000x1000_100.pdf}}\\
\subcaptionbox{2000x2000 Matrix with 10 Iterations\label{saturn:cilk:2000:10}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_2000x2000_10.pdf}} &
\subcaptionbox{2000x2000 Matrix with 100 Iterations\label{saturn:cilk:2000:100}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_2000x2000_100.pdf}}\\
\subcaptionbox{6000x6000 Matrix with 10 Iterations\label{saturn:cilk:6000:10}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_6000x6000_10.pdf}}
\end{tabular}
\end{figure}

\begin{figure}[H] 
\caption{MPI benchmark on Saturn}
\begin{tabular}{cc}
\subcaptionbox{1000x1000 Matrix with 10 Iterations\label{saturn:mpi:1000:10}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_1000x1000_10.pdf}} &
\subcaptionbox{1000x1000 Matrix with 100 Iterations\label{saturn:mpi:1000:100}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_1000x1000_100.pdf}}\\
\subcaptionbox{2000x2000 Matrix with 10 Iterations\label{saturn:mpi:2000:10}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_2000x2000_10.pdf}} &
\subcaptionbox{2000x2000 Matrix with 100 Iterations\label{saturn:mpi:2000:100}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_2000x2000_100.pdf}}\\
\subcaptionbox{6000x6000 Matrix with 10 Iterations\label{saturn:mpi:6000:10}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_6000x6000_10.pdf}}
\end{tabular}
\end{figure}

\subsection{Jupiter}
Jupiter is a distributed memory parallel computer cluster with 36 computing nodes interconnected by a InfiniBand network. Each computing node has 16 AMD CPU cores.

\begin{table}[H]
  \label{tab:jupiter} 
  \caption{Hard- and software configuration of Jupiter}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      Computing nodes & 36\\\hline
      CPUs per node & 2 AMD Opteron 6134 (8 cores, 2.3 GHz, 12 MB cache)\\\hline
      Main Memory per node & 32 GB DDR3-1333\\\hline
      Interconnection network & QDR InfiniBand and Gigabit Ethernet\\\hline
      Operating system & Linux 64 bit (CentOS 6)\\\hline
      Compiler & gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-16)\\\hline
      MPI & MPICH 3.0.4\\\hline
    \end{tabular}    
  \end{center}
\end{table}

The hard- and software configuration of the test machine can be seen in table \ref{tab:jupiter}. As compiler options we used \verb|-O3|, \verb|-std=gnu99| and \verb|-msse2| in addition to the default compiler flags provided by the individual frameworks. We enabled \verb|-DNDEBUG| while we did the benchmarks. The host file was generated by \verb|echo -e jupiter{0..35}\\n > hosts| and passed on to \verb|mpiexec| via the \verb|hostfile| flag.\\
\\
On Jupiter we compared the sequential and MPI stencil implementations. We measured the average execution time of each stencil implementation by executing each of them 30 times in a row. The problem sizes are a variation between different matrix sizes and different number of iterations. We took 2000x2000, 4000x4000, 10000x10000 and 20000x20000 as matrix sizes for our benchmark, and we applied the stencil operator on each matrix size with 5 and 10 iterations.

\begin{figure}[H] 
\caption{Sequential benchmark on Jupiter}
\begin{tabular}{cc}
\subcaptionbox{5 Iterations\label{jupiter:seq:5}}{\includegraphics[width=0.5\textwidth]{jupiter_seq_5.pdf}} &
\subcaptionbox{10 Iterations\label{jupiter:seq:10}}{\includegraphics[width=0.5\textwidth]{jupiter_seq_10.pdf}}
\end{tabular}
\end{figure}

\begin{figure}[H] 
\caption{MPI benchmark on Jupiter}
\begin{tabular}{cc}
\subcaptionbox{2000x2000 Matrix with 5 Iterations\label{jupiter:mpi:2000:5}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_2000x2000_5.pdf}} &
\subcaptionbox{2000x2000 Matrix with 10 Iterations\label{jupiter:mpi:2000:10}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_2000x2000_10.pdf}}\\
\subcaptionbox{4000x4000 Matrix with 5 Iterations\label{jupiter:mpi:4000:5}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_4000x4000_5.pdf}} &
\subcaptionbox{4000x4000 Matrix with 10 Iterations\label{jupiter:mpi:4000:10}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_4000x4000_10.pdf}}\\
\subcaptionbox{10000x10000 Matrix with 5 Iterations\label{jupiter:mpi:10000:5}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_10000x10000_5.pdf}} &
\subcaptionbox{10000x10000 Matrix with 10 Iterations\label{jupiter:mpi:10000:10}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_10000x10000_10.pdf}}\\
\subcaptionbox{20000x20000 Matrix with 5 Iterations\label{jupiter:mpi:20000:5}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_20000x20000_5.pdf}} &
\subcaptionbox{20000x20000 Matrix with 10 Iterations\label{jupiter:mpi:20000:10}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_20000x20000_10.pdf}}
\end{tabular}
\end{figure}

\section{Conclusion}
% comparison between openmp, cilk and mpi

\bibliographystyle{plain}
\bibliography{references}

\end{document}
