\documentclass[11pt,a4paper]{article}
\usepackage[top=2cm, left=2cm, right=2cm, bottom=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}

\title{2D Stencil - Parallel Approaches}
\author{
  Emmanuel Pescosta\\
  1326934 
  \and
  Philipp Paris\\
  1325664
}
\date{\today}

\begin{document}

\maketitle

\section{Preliminaries}

\subsection{Stencil Computation}
Stencil computations update each value of an n-dimensional array as a function of the corresponding neighbourhood values. Equation \ref{eq:stencil} shows an example neighbourhood function for a 2-dimensional array.
\begin{equation}\label{eq:stencil}
 A[i,j] = \frac{A[i-1,j] + A[i+1,j] + A[i,j-1] + A[i,j+1]}{4}
\end{equation}
In a single stencil computation this process of updating all values of the array is usually repeated a certain number of times, called iterations. \\
Special measures have to be taken to update the boundary values of the array, which lack one or more neighbour values: there exist many different ways and in this project we will simply skip the updating of the boundary values.

\subsection{Aim of the project}
In this project we compare different implementations of a Five-Point-2D-Stencil (equation \ref{eq:stencil}) in their memory consumption and runtime. We will look at sequential and parallel implementations using Cilk, OpenMP and MPI.\\
The aim is to achieve the best possible speedup of the calculation using parallel algorithms in comparison to the sequential implementation while keeping the memory consumption as low as possible. Furthermore we want to to analyze the impact of the memory usage on the runtime of the calculation.\\
Another intention is to analyze cache-aware and cache-unaware implementations of the stencil-computation in their runtime. 

\section{Sequential}
\subsection{Memory-infficient Implementation}
% with temporary matrix
\subsection{Memory-efficient Implementation}
% with single vector buffer

\section{Cilk}

% task-parallel formulation of the stencil update, compare with openmp solution

\section{OpenMP}
\subsection{Classical Implementation}
% temporary matrix, not thread local
\subsection{Row-wise Implementation}
% row-wise update: thread local vs non thread local 
\subsection{Column-wise Implementation}
% column-wise update: thread local vs non thread local 

\section{MPI}
% matrix is distributed as n/r x m/c
% give a theoretical speedup estimate: How should r and c be chosen for best performance
\subsection{Blocking Boundary Exchange}
% sendrecv
\subsection{Point-To-Point Boundary Exchange}
%non-blocking point-to-point
\subsection{One-Sided Boundary Exchange}
%one sided

\section{Testing}
% ctest

\section{Results}

\subsection{Saturn}
Saturn is a shared memory parallel computer with 48 AMD CPU cores.

\begin{table}[H]
  \label{tab:saturn} 
  \caption{Hard- and software configuration of Saturn}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      CPUs & 4 AMD Opteron 6168 (12 cores, 1.9 GHz, 12 MB cache)\\\hline
      Main Memory & 128 GB DDR3-1333\\\hline
      Operating system & Linux 64 bit (Debian Testing)\\\hline
      Compiler & gcc (Debian 5.2.1-23) 5.2.1 20151028\\\hline
      Cilk & 5.4.6\\\hline
      MPI & Open MPI 1.8.4\\\hline
    \end{tabular}
  \end{center}
\end{table}

The hard- and software configuration of the test machine can be seen in table \ref{tab:saturn}. As compiler options we used \verb|-Ofast|, \verb|-std=gnu99| and \verb|-msse2| in addition to the default compiler flags provided by the individual frameworks. We enabled \verb|-DNDEBUG| and thread pinning for OpenMP, by exporting \verb|OMP_PROC_BIND=true|, while we did the benchmarks.\\
\\
On Saturn we compared the sequential, OpenMP, Cilk and MPI stencil implementations:

\begin{figure}[H] 
\caption{Sequential benchmark on Saturn}
\begin{tabular}{cc}
\subcaptionbox{1000x1000 Matrix\label{saturn:seq:1000}}{\includegraphics[width=0.5\textwidth]{saturn_seq_1000x1000.pdf}} &
\subcaptionbox{2000x2000 Matrix\label{saturn:seq:2000}}{\includegraphics[width=0.5\textwidth]{saturn_seq_2000x2000.pdf}}\\
\subcaptionbox{6000x6000 Matrix\label{saturn:seq:6000}}{\includegraphics[width=0.5\textwidth]{saturn_seq_6000x6000.pdf}} 
\end{tabular}
\end{figure}

\begin{figure}[H] 
\caption{OpenMP benchmark on Saturn}
\begin{tabular}{cc}
\subcaptionbox{1000x1000 Matrix with 1 Iterations\label{saturn:openmp:1000:1}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_1000x1000_1.pdf}} &
\subcaptionbox{1000x1000 Matrix with 10 Iterations\label{saturn:openmp:1000:10}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_1000x1000_10.pdf}}\\
\subcaptionbox{1000x1000 Matrix with 100 Iterations\label{saturn:openmp:1000:100}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_1000x1000_100.pdf}} &
\subcaptionbox{2000x2000 Matrix with 1 Iterations\label{saturn:openmp:2000:1}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_2000x2000_1.pdf}}\\
\subcaptionbox{2000x2000 Matrix with 10 Iterations\label{saturn:openmp:2000:10}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_2000x2000_10.pdf}} &
\subcaptionbox{2000x2000 Matrix with 100 Iterations\label{saturn:openmp:2000:100}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_2000x2000_100.pdf}}\\
\subcaptionbox{6000x6000 Matrix with 1 Iterations\label{saturn:openmp:6000:1}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_6000x6000_1.pdf}} &
\subcaptionbox{6000x6000 Matrix with 10 Iterations\label{saturn:openmp:6000:10}}{\includegraphics[width=0.5\textwidth]{saturn_openmp_6000x6000_10.pdf}}
\end{tabular}
\end{figure}

\begin{figure}[H] 
\caption{Cilk benchmark on Saturn}
\begin{tabular}{cc}
\subcaptionbox{1000x1000 Matrix with 1 Iterations\label{saturn:cilk:1000:1}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_1000x1000_1.pdf}} &
\subcaptionbox{1000x1000 Matrix with 10 Iterations\label{saturn:cilk:1000:10}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_1000x1000_10.pdf}}\\
\subcaptionbox{1000x1000 Matrix with 100 Iterations\label{saturn:cilk:1000:100}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_1000x1000_100.pdf}} &
\subcaptionbox{2000x2000 Matrix with 1 Iterations\label{saturn:cilk:2000:1}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_2000x2000_1.pdf}}\\
\subcaptionbox{2000x2000 Matrix with 10 Iterations\label{saturn:cilk:2000:10}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_2000x2000_10.pdf}} &
\subcaptionbox{2000x2000 Matrix with 100 Iterations\label{saturn:cilk:2000:100}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_2000x2000_100.pdf}}\\
\subcaptionbox{6000x6000 Matrix with 1 Iterations\label{saturn:cilk:6000:1}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_6000x6000_1.pdf}} &
\subcaptionbox{6000x6000 Matrix with 10 Iterations\label{saturn:cilk:6000:10}}{\includegraphics[width=0.5\textwidth]{saturn_cilk_6000x6000_10.pdf}}
\end{tabular}
\end{figure}

\begin{figure}[H] 
\caption{MPI benchmark on Saturn}
\begin{tabular}{cc}
\subcaptionbox{1000x1000 Matrix with 1 Iterations\label{saturn:mpi:1000:1}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_1000x1000_1.pdf}} &
\subcaptionbox{1000x1000 Matrix with 10 Iterations\label{saturn:mpi:1000:10}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_1000x1000_10.pdf}}\\
\subcaptionbox{1000x1000 Matrix with 100 Iterations\label{saturn:mpi:1000:100}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_1000x1000_100.pdf}} &
\subcaptionbox{2000x2000 Matrix with 1 Iterations\label{saturn:mpi:2000:1}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_2000x2000_1.pdf}}\\
\subcaptionbox{2000x2000 Matrix with 10 Iterations\label{saturn:mpi:2000:10}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_2000x2000_10.pdf}} &
\subcaptionbox{2000x2000 Matrix with 100 Iterations\label{saturn:mpi:2000:100}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_2000x2000_100.pdf}}\\
\subcaptionbox{6000x6000 Matrix with 1 Iterations\label{saturn:mpi:6000:1}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_6000x6000_1.pdf}} &
\subcaptionbox{6000x6000 Matrix with 10 Iterations\label{saturn:mpi:6000:10}}{\includegraphics[width=0.5\textwidth]{saturn_mpi_6000x6000_10.pdf}}
\end{tabular}
\end{figure}

\subsection{Jupiter}
Jupiter is a distributed memory parallel computer cluster with 36 computing nodes interconnected by a InfiniBand network. Each computing node has 16 AMD CPU cores.

\begin{table}[H]
  \label{tab:jupiter} 
  \caption{Hard- and software configuration of Jupiter}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      Computing nodes & 36\\\hline
      CPUs per node & 2 AMD Opteron 6134 (8 cores, 2.3 GHz, 12 MB cache)\\\hline
      Main Memory per node & 32 GB DDR3-1333\\\hline
      Interconnection network & QDR InfiniBand and Gigabit Ethernet\\\hline
      Operating system & Linux 64 bit (CentOS 6)\\\hline
      Compiler & gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-16)\\\hline
      MPI & MPICH 3.0.4\\\hline
    \end{tabular}    
  \end{center}
\end{table}

The hard- and software configuration of the test machine can be seen in table \ref{tab:jupiter}. As compiler options we used \verb|-O3|, \verb|-std=gnu99| and \verb|-msse2| in addition to the default compiler flags provided by the individual frameworks. We enabled \verb|-DNDEBUG| while we did the benchmarks. The host file was generated by \verb|echo -e jupiter{0..35}\\n > hosts| and passed on to \verb|mpiexec| via the \verb|hostfile| flag.\\
\\
On Jupiter we compared the sequentl and MPI stencil implementations.

\begin{figure}[H] 
\caption{Sequential benchmark on Jupiter}
\begin{tabular}{cc}
\subcaptionbox{2000x2000 Matrix\label{jupiter:seq:2000}}{\includegraphics[width=0.5\textwidth]{jupiter_seq_2000x2000.pdf}} &
\subcaptionbox{4000x4000 Matrix\label{jupiter:seq:4000}}{\includegraphics[width=0.5\textwidth]{jupiter_seq_4000x4000.pdf}}\\
\subcaptionbox{10000x10000 Matrix\label{jupiter:seq:10000}}{\includegraphics[width=0.5\textwidth]{jupiter_seq_10000x10000.pdf}} &
\subcaptionbox{20000x20000 Matrix\label{jupiter:seq:20000}}{\includegraphics[width=0.5\textwidth]{jupiter_seq_20000x20000.pdf}}
\end{tabular}
\end{figure}

\begin{figure}[H] 
\caption{MPI benchmark on Jupiter}
\begin{tabular}{cc}
\subcaptionbox{2000x2000 Matrix with 5 Iterations\label{jupiter:mpi:2000:5}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_2000x2000_5.pdf}} &
\subcaptionbox{2000x2000 Matrix with 10 Iterations\label{jupiter:mpi:2000:10}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_2000x2000_10.pdf}}\\
\subcaptionbox{4000x4000 Matrix with 5 Iterations\label{jupiter:mpi:4000:5}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_4000x4000_5.pdf}} &
\subcaptionbox{4000x4000 Matrix with 10 Iterations\label{jupiter:mpi:4000:10}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_4000x4000_10.pdf}}\\
\subcaptionbox{10000x10000 Matrix with 5 Iterations\label{jupiter:mpi:10000:5}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_10000x10000_5.pdf}} &
\subcaptionbox{10000x10000 Matrix with 10 Iterations\label{jupiter:mpi:10000:10}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_10000x10000_10.pdf}}\\
\subcaptionbox{20000x20000 Matrix with 5 Iterations\label{jupiter:mpi:20000:5}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_20000x20000_5.pdf}} &
\subcaptionbox{20000x20000 Matrix with 10 Iterations\label{jupiter:mpi:20000:10}}{\includegraphics[width=0.5\textwidth]{jupiter_mpi_20000x20000_10.pdf}}
\end{tabular}
\end{figure}

\section{Conclusion}
% comparison between openmp, cilk and mpi

\end{document}
